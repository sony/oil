{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from online.envs.bidding_env import BiddingEnv\n",
    "from definitions import ROOT_DIR\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"/home/ubuntu/Dev/NeurIPS_Auto_Bidding_General_Track_Baseline/output/testing/026_onbc_seed_0_new_data_realistic_60_obs_resume_023/dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def transform_dataframe(df):\n",
    "    new_data = {\n",
    "        'norm_obs': [],\n",
    "        'pvalue': [],\n",
    "        'pvalue_sigma': [],\n",
    "        'oracle_action': [],\n",
    "        'episode': [],\n",
    "        'step': []\n",
    "    }\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # Extract row-specific data\n",
    "        norm_obs = row['norm_obs']\n",
    "        pvalues = row['pvalues']\n",
    "        pvalues_sigma = row['pvalues_sigma']\n",
    "        oracle_action = row['oracle_action']\n",
    "        episode = row['episode']\n",
    "        step = row['step']\n",
    "\n",
    "        # Iterate through each pvalue\n",
    "        for i in range(len(pvalues)):\n",
    "            # Append norm_obs, pvalue, pvalue_sigma and oracle_action\n",
    "            new_data['norm_obs'].append(norm_obs)\n",
    "            new_data['pvalue'].append(pvalues[i])\n",
    "            new_data['pvalue_sigma'].append(pvalues_sigma[i])\n",
    "            new_data['oracle_action'].append(oracle_action[i])\n",
    "            new_data['episode'].append(episode)\n",
    "            new_data['step'].append(step)\n",
    "\n",
    "    # Create new transformed DataFrame\n",
    "    transformed_df = pd.DataFrame(new_data)\n",
    "    return transformed_df\n",
    "\n",
    "# Assuming your DataFrame is called `df`\n",
    "transformed_df = transform_dataframe(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split by episode\n",
    "unique_episodes = transformed_df['episode'].unique()\n",
    "\n",
    "# Split into 80% train and 20% test\n",
    "train_episodes, test_episodes = train_test_split(unique_episodes, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create train and test sets based on the episode split\n",
    "train_df = transformed_df[transformed_df['episode'].isin(train_episodes)]\n",
    "test_df = transformed_df[transformed_df['episode'].isin(test_episodes)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class BiddingDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the row\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        # Prepare the input (norm_obs + pvalue + pvalue_sigma)\n",
    "        norm_obs = np.array(row['norm_obs'], dtype=np.float32)\n",
    "        pvalue = np.array(row['pvalue'], dtype=np.float32)\n",
    "        pvalue_sigma = np.array(row['pvalue_sigma'], dtype=np.float32)\n",
    "\n",
    "        # Concatenate inputs\n",
    "        input_features = np.concatenate([norm_obs, [pvalue], [pvalue_sigma]])\n",
    "\n",
    "        # Oracle action as the target\n",
    "        target = np.array(row['oracle_action'], dtype=np.float32)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        return torch.tensor(input_features), torch.tensor(target)\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = BiddingDataset(train_df)\n",
    "test_dataset = BiddingDataset(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set up DataLoader for batching\n",
    "batch_size = 64  # Example batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple feedforward neural network\n",
    "class BidRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(BidRegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)  # Output is a single value (the oracle action)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Initialize the model, optimizer, and loss function\n",
    "input_size = 60 + 2  # norm_obs length + pvalue + pvalue_sigma\n",
    "model = BidRegressionModel(input_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()  # You could also experiment with HuberLoss\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, test_loader, num_epochs=20):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.unsqueeze(1))\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Print training loss for the epoch\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets.unsqueeze(1))\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        print(f'Test Loss: {test_loss/len(test_loader)}')\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alibaba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
